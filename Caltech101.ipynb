{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNG8Da93y36hvq5f5gjCLNa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4381d970ca8b4b839ad2f4bebba2ac5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ea02427253e4c6a859e9c25ed5d30bd",
              "IPY_MODEL_b965c7ff24fd426fa70b13dfd12bb3a3",
              "IPY_MODEL_d1772f7e65cf4024985742cfbba1fe75"
            ],
            "layout": "IPY_MODEL_e60823f17bdf4fbeaf2890e6d274b47b"
          }
        },
        "8ea02427253e4c6a859e9c25ed5d30bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67e0ccce669e4308850432721160c192",
            "placeholder": "​",
            "style": "IPY_MODEL_f974a9d67e854545862265b71fd10e0d",
            "value": "100%"
          }
        },
        "b965c7ff24fd426fa70b13dfd12bb3a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ada1a26c27d46f59fe2b4ff6131e82c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77d5ea6c55ba462d9dfe2a8067ff02d8",
            "value": 1
          }
        },
        "d1772f7e65cf4024985742cfbba1fe75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79f043c95dfa49aeae24cd900d7e4291",
            "placeholder": "​",
            "style": "IPY_MODEL_3ad593ed7eba401fbf4dd857dd851482",
            "value": " 1/1 [00:00&lt;00:00,  3.60it/s]"
          }
        },
        "e60823f17bdf4fbeaf2890e6d274b47b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67e0ccce669e4308850432721160c192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f974a9d67e854545862265b71fd10e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ada1a26c27d46f59fe2b4ff6131e82c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d5ea6c55ba462d9dfe2a8067ff02d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79f043c95dfa49aeae24cd900d7e4291": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad593ed7eba401fbf4dd857dd851482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enVives/Caltech101/blob/main/Caltech101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "TRqBiAE19FY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682fd09f-fb4b-46c7-f88b-de5e71535f38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.cuda.Stream device=cuda:0 cuda_stream=0x0>"
            ]
          },
          "metadata": {},
          "execution_count": 325
        }
      ],
      "source": [
        "from itertools import filterfalse\n",
        "from collections import OrderedDict\n",
        "#!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "import torch,torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import cv2\n",
        "import time\n",
        "import os\n",
        "import wandb\n",
        "import shutil\n",
        "import pylab as pl\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms,models\n",
        "from torch import nn\n",
        "from IPython.display import clear_output,display\n",
        "from torch.utils.data import DataLoader, random_split,Subset,Dataset\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from glob import glob\n",
        "\n",
        "ROOT = './sample_data/'\n",
        "CLASS_A = 'cougar_body'\n",
        "CLASS_B = 'windsor_chair'\n",
        "\n",
        "architectures = {'alexnet': 0,'vgg': 1,'resnet': 2,'unet':3,'propi':4,'yolo':5}\n",
        "MODEL = architectures['propi']\n",
        "\n",
        "ENTRENAR = False\n",
        "ENLLAC_PESOS = '/content/propi.pt'\n",
        "WEIGHTSANDBIASES = False\n",
        "\n",
        "DOWNLOAD = False\n",
        "DUPLICAR_DADES = True\n",
        "\n",
        "if WEIGHTSANDBIASES:\n",
        "  wandb.login()\n",
        "\n",
        "\n",
        "#471be466c8949671a46c67e7aad0d5a0ac8c9dad\n",
        "#!rm -rf /content/sample_data/*\n",
        "\n",
        "\n",
        "torch.cuda.default_stream(torch.device('cuda'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mètode utilitzat per obtenir la mitja i la std de totes les imatges\n",
        "#Aquestes s'utilitzaràn a les transformacions\n",
        "\n",
        "def cerca_mitja_std(dataset):\n",
        "  dataloader = DataLoader(dataset,batch_size = 1,shuffle=False)\n",
        "  mitja = torch.zeros(3)\n",
        "  std = torch.zeros(3)\n",
        "  n = 0\n",
        "\n",
        "  for image,_ in dataloader:\n",
        "    mitja += image.mean(dim=[0, 2, 3])\n",
        "    std += image.std(dim=[0, 2, 3])\n",
        "    n += 1\n",
        "\n",
        "  mitja /= n\n",
        "  std /= n\n",
        "\n",
        "  print(mitja)\n",
        "  print(std)"
      ],
      "metadata": {
        "id": "4JYb0AOJbCdC"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import NONE\n",
        "#Classe on guardam els paths,labels i transformacions dels conjunts de training,validation i testing\n",
        "class Formes(Dataset):\n",
        "    def __init__(self, paths, labels= None, transforms = None,annotations = None,transform_mask = None):\n",
        "        self.images = paths\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms # per a transformar les imatges\n",
        "        self.annotations = annotations #conté la informació adicional de les imatges\n",
        "        self.transforms_mask = transform_mask #conté les transformacions que aplicarem a les màsqueres\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __setmodel__(self,model):\n",
        "        self.model = model\n",
        "\n",
        "    def __getdist__(self): #distribució de les classes\n",
        "      return pd.Series(self.labels).value_counts()\n",
        "\n",
        "    def __showcontours__(self,index): #mètode utilitzat per a mostrar el contorn d'una imatge guardat a annotations (no utilitzat ja)\n",
        "      path = self.images[index]\n",
        "      image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "      image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      plt.imshow(image)\n",
        "\n",
        "      mat_data = scipy.io.loadmat(self.annotations[index])\n",
        "        #print(\"Keys in MAT file:\", polygons_data.keys())\n",
        "\n",
        "      polygons_data = mat_data['obj_contour']\n",
        "\n",
        "      x_points = polygons_data[0]\n",
        "      y_points = polygons_data[1]\n",
        "\n",
        "      contour_points = np.array(list(zip(x_points,y_points)))\n",
        "\n",
        "      plt.plot(contour_points[:, 0], contour_points[:, 1], '-r')  # Red contour line\n",
        "      plt.scatter(contour_points[:, 0], contour_points[:, 1], c='blue')  # Optional: Mark contour points\n",
        "      plt.show()\n",
        "\n",
        "    def __getpureimage__(self,index): #obtenir una imatge fora transformacions (no utilitzat ja)\n",
        "      path = self.images[index]\n",
        "      image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "      image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "      return image\n",
        "\n",
        "    def __getitem__(self, index): #obtenir\n",
        "      path = self.images[index]\n",
        "      label = self.labels[index]\n",
        "\n",
        "      seed = torch.random.seed()  # Ccream una seed que utilitzarem per a les transformacions\n",
        "\n",
        "      if self.annotations != None:\n",
        "        mat_data = scipy.io.loadmat(self.annotations[index])\n",
        "\n",
        "\n",
        "        polygons_data = mat_data['obj_contour']\n",
        "        boundingbox_data = mat_data['box_coord']\n",
        "\n",
        "        # print(\"Keys in MAT file:\", mat_data.keys())\n",
        "\n",
        "        x_points = polygons_data[0]\n",
        "        y_points = polygons_data[1]\n",
        "\n",
        "        contour_points = list(zip(x_points,y_points))\n",
        "        contour = np.array(contour_points, dtype=np.int32)\n",
        "\n",
        "        #print(contour_points)\n",
        "\n",
        "\n",
        "      image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "      image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      if self.model == 3: #si esteim utilitzant la unet\n",
        "        mask = np.zeros(image.size, dtype=np.uint8)\n",
        "        cv2.fillPoly(mask, [contour], color=255)  # Pintam de blanc el polígon que repersenta la mask\n",
        "        mask = Image.fromarray(mask)  #convertim el numpy a PIL image\n",
        "\n",
        "        torch.random.manual_seed(seed)  # ja que cada transformació es random, ens volem assegurad de que la imatge té la mateixa transformació que la mask\n",
        "        mask_resized = self.transforms_mask(mask)\n",
        "        mask_resized = (mask_resized > 0.5).float()\n",
        "\n",
        "        torch.random.manual_seed(seed)  # mateixa seed per a la imatge\n",
        "        image = self.transforms(image)\n",
        "\n",
        "        return image, mask_resized\n",
        "\n",
        "      image = self.transforms(image)\n",
        "\n",
        "      return image, label"
      ],
      "metadata": {
        "id": "8imK0xLhEnm1"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mètode utilitzat per afegir la informació als fitxers txt del dataset que utilitzarà el model YOLO\n",
        "\n",
        "def escriu_informacio_yolo(carpeta,nom_imatge,annotations,class_id,image_size):\n",
        "\n",
        "    label_file = os.path.join(carpeta, f\"{os.path.splitext(nom_imatge)[0]}.txt\")\n",
        "    with open(label_file, \"w\") as f:\n",
        "            y_min, y_max, x_min, x_max = annotations[0]\n",
        "\n",
        "            if x_max > image_size[0]: #ens hem trobat qualque imatge que la y_max o la x_max superava les dimensions de la imatge\n",
        "              x_max = image_size[0]\n",
        "            if y_max > image_size[1]:\n",
        "              y_max = image_size[1]\n",
        "\n",
        "            #calculam el centre del bounding box\n",
        "            if image_size:\n",
        "                img_width, img_height = image_size\n",
        "                x_center = ((x_min + x_max) / 2) / img_width\n",
        "                y_center = ((y_min + y_max) / 2) / img_height\n",
        "                width = (x_max - x_min) / img_width\n",
        "                height = (y_max - y_min) / img_height\n",
        "            else:\n",
        "                x_center = (x_min + x_max) / 2\n",
        "                y_center = (y_min + y_max) / 2\n",
        "                width = x_max - x_min\n",
        "                height = y_max - y_min\n",
        "\n",
        "            # Escrivim la informació a l'arxiu\n",
        "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")"
      ],
      "metadata": {
        "id": "T-Xip5ijGPqX"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TRAINING = 0.80\n",
        "VAL = 0.10\n",
        "TESTING = 0.10\n",
        "\n",
        "mean = torch.tensor([0.485,0.456,0.406])\n",
        "std = torch.tensor([0.229,0.224,0.225])\n",
        "\n",
        "#transform per a les imatges\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "#transform per a les màsqueres\n",
        "transform_masks = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "#transform per al conjunt de training\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.Caltech101(root= ROOT,download=DOWNLOAD,transform=transform)\n",
        "\n",
        "img_class_1 = sorted(glob('/content/sample_data/caltech101/101_ObjectCategories/cougar_body/*'))\n",
        "img_class_2 = sorted(glob('/content/sample_data/caltech101/101_ObjectCategories/windsor_chair/*'))\n",
        "\n",
        "img_annotations_class_1 = sorted(glob('/content/sample_data/caltech101/Annotations/cougar_body/*'))\n",
        "img_annotations_class_2 = sorted(glob('/content/sample_data/caltech101/Annotations/windsor_chair/*'))\n",
        "\n",
        "\n",
        "img_files = img_class_1 +  img_class_2\n",
        "img_annotations = img_annotations_class_1 + img_annotations_class_2\n",
        "\n",
        "#si volem duplicar el nombre d'imatges\n",
        "if DUPLICAR_DADES:\n",
        "  img_files_duplicated = img_files + img_files  # Duplicate image paths\n",
        "  img_annotations_duplicated = img_annotations + img_annotations  # Duplicate annotation paths\n",
        "\n",
        "  img_files = img_files_duplicated\n",
        "  img_annotations = img_annotations_duplicated\n",
        "\n",
        "labels = []\n",
        "for img_path in img_files:\n",
        "  label = img_path.split(os.path.sep)[-2]\n",
        "  labels.append(label)\n",
        "#codificam les labels\n",
        "lb = LabelEncoder()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test, annotations_train, annotations_test = train_test_split(\n",
        "    img_files, labels, img_annotations, test_size=TESTING, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val, annotations_train, annotations_val = train_test_split(\n",
        "    X_train, y_train, annotations_train, test_size=VAL/(TRAINING+VAL), random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "training = Formes(X_train,y_train,transform2,annotations_train,transform_masks)\n",
        "testing = Formes(X_test,y_test,transform,annotations_test,transform_masks)\n",
        "validation = Formes(X_val,y_val,transform,annotations_val,transform_masks)\n",
        "\n",
        "#només el primer pic\n",
        "if DOWNLOAD:\n",
        "\n",
        "  #Preparar informació per YOLO\n",
        "\n",
        "  carpeta_yolo = \"dataset\"\n",
        "\n",
        "  if not os.path.exists(carpeta_yolo):\n",
        "      os.mkdir(carpeta_yolo)\n",
        "\n",
        "  #cream l'estructura que utilitzarà yolo de ultralitics\n",
        "\n",
        "  directoris = [\"dataset/train/\",\"dataset/test/\",\"dataset/val/\",\"dataset/train/images/\",\"dataset/train/labels/\",\"dataset/val/images/\",\"dataset/val/labels/\",\"dataset/test/images/\",\"dataset/test/labels/\"]\n",
        "\n",
        "  for carpeta in directoris:\n",
        "    if not os.path.exists(carpeta):\n",
        "      os.mkdir(carpeta)\n",
        "\n",
        "  for elemento in X_train:\n",
        "    shutil.copy(elemento, \"/content/dataset/train/images\")\n",
        "\n",
        "  for elemento in X_val:\n",
        "    shutil.copy(elemento, \"/content/dataset/val/images\")\n",
        "\n",
        "  for elemento in X_test:\n",
        "    shutil.copy(elemento, \"/content/dataset/test/images\")\n",
        "\n",
        "  #ara a les carpetes de labels training, validation i testing hi afegirem la informació:\n",
        "  #class_id,x_center,y_center,width,height de cada imatge que es el que requereix YOLOv5\n",
        "\n",
        "  for i in range(len(X_train)):\n",
        "    mat_data = scipy.io.loadmat(annotations_train[i])\n",
        "    nom = X_train[i].split(\"/\")[-1]\n",
        "    label = y_train[i]\n",
        "    boundingbox_data = mat_data['box_coord']\n",
        "    width,height = Image.open(X_train[i]).size\n",
        "\n",
        "    escriu_informacio_yolo(\"dataset/train/labels/\",nom,boundingbox_data,label,(width,height))\n",
        "\n",
        "  for i in range(len(X_val)):\n",
        "    mat_data = scipy.io.loadmat(annotations_val[i])\n",
        "    nom = X_val[i].split(\"/\")[-1]\n",
        "    label = y_val[i]\n",
        "    boundingbox_data = mat_data['box_coord']\n",
        "    width,height = Image.open(X_val[i]).size\n",
        "\n",
        "    escriu_informacio_yolo(\"dataset/val/labels/\",nom,boundingbox_data,label,(width,height))\n",
        "\n",
        "  for i in range(len(X_test)):\n",
        "    mat_data = scipy.io.loadmat(annotations_test[i])\n",
        "    nom = X_test[i].split(\"/\")[-1]\n",
        "    label = y_test[i]\n",
        "    boundingbox_data = mat_data['box_coord']\n",
        "    width,height = Image.open(X_test[i]).size\n",
        "\n",
        "    escriu_informacio_yolo(\"dataset/test/labels/\",nom,boundingbox_data,label,(width,height))\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "5e-PEAT6NDpO"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"LONGITUD SET DE TRAINING: {training.__len__()}\")\n",
        "with open(\"training.txt\", \"w\") as file:\n",
        "    for item in training.__getdist__()/training.__len__():\n",
        "        file.write(f\"{item}\\n\")\n",
        "print(f\"LONGITUD SET DE VALIDATION: {validation.__len__()}\")\n",
        "with open(\"valid.txt\", \"w\") as file:\n",
        "    for item in validation.__getdist__()/validation.__len__():\n",
        "        file.write(f\"{item}\\n\")\n",
        "print(f\"LONGITUD SET DE TESTING: {testing.__len__()}\")"
      ],
      "metadata": {
        "id": "npbH2OEvRSVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5eb420a-ee0d-4fe4-f507-9c5aa239095e"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LONGITUD SET DE TRAINING: 164\n",
            "LONGITUD SET DE VALIDATION: 21\n",
            "LONGITUD SET DE TESTING: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ara ja sabem que hem de predir si una imatge pertany a les classes 25 o 99"
      ],
      "metadata": {
        "id": "riWfhpfjmgeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model UNET utilitzat a classe, jo només hi he afegit el dropout per a fer proves\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        dropout_prob = 0.0\n",
        "\n",
        "        ## CODER\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\", dropout_prob=dropout_prob)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\", dropout_prob=dropout_prob)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\", dropout_prob=dropout_prob)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\", dropout_prob=dropout_prob)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\", dropout_prob=dropout_prob)\n",
        "\n",
        "        ## DECODER\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2,\n",
        "                                          stride=2)  # Empra aquesta capa com exemple\n",
        "        self.decoder4 = UNet._block(features * 16, features * 8, name=\"dec4\",dropout_prob=0)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = UNet._block(features * 8, features * 4, name=\"dec3\",dropout_prob=0)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = UNet._block(features * 4, features * 2, name=\"dec2\",dropout_prob=0)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\",dropout_prob=0)\n",
        "\n",
        "        self.final = nn.Conv2d(\n",
        "            in_channels=features,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=1,\n",
        "            padding=0,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec1 = self.upconv4(bottleneck)\n",
        "        dec1 = torch.cat((dec1, enc4), dim=1)\n",
        "        dec2 = self.decoder4(dec1)\n",
        "\n",
        "        dec2 = self.upconv3(dec2)\n",
        "        dec2 = torch.cat((dec2, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec2)\n",
        "\n",
        "        dec3 = self.upconv2(dec3)\n",
        "        dec3 = torch.cat((dec3, enc2), dim=1)\n",
        "        dec4 = self.decoder2(dec3)\n",
        "\n",
        "        dec4 = self.upconv1(dec4)\n",
        "        dec4 = torch.cat((dec4, enc1), dim=1)\n",
        "        dec5 = self.decoder1(dec4)\n",
        "        return torch.sigmoid(self.final(dec5))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name,dropout_prob):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (name + \"conv1\",\n",
        "                     nn.Conv2d(\n",
        "                         in_channels=in_channels,\n",
        "                         out_channels=features,\n",
        "                         kernel_size=3,\n",
        "                         padding=1,\n",
        "                         bias=False,\n",
        "                     ),\n",
        "                     ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (name + \"dropout1\", nn.Dropout2d(p=dropout_prob)),\n",
        "\n",
        "                    (name + \"conv2\",\n",
        "                     nn.Conv2d(\n",
        "                         in_channels=features,\n",
        "                         out_channels=features,\n",
        "                         kernel_size=3,\n",
        "                         padding=1,\n",
        "                         bias=False,\n",
        "                     ),\n",
        "                     ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                    (name + \"dropout2\", nn.Dropout2d(p=dropout_prob)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "id": "pUYwQPDSev7e"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Diceloss utilitzat de classe\n",
        "class DiceLoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = 0.0\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        assert y_pred.size() == y_true.size()\n",
        "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
        "        y_true = y_true[:, 0].contiguous().view(-1)\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dsc = (2. * intersection + self.smooth) / (\n",
        "                y_pred.sum() + y_true.sum() + self.smooth\n",
        "        )\n",
        "        return 1. - dsc"
      ],
      "metadata": {
        "id": "Mi_tP2rWd50b"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pick_algorithm(number):\n",
        "  if number == 0:\n",
        "    if ENTRENAR:\n",
        "      alexnet = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1) #fine tuning\n",
        "    else:\n",
        "      alexnet = models.alexnet()\n",
        "\n",
        "    alexnet.classifier[6] = nn.Linear(in_features=4096, out_features=1)\n",
        "\n",
        "    if ENTRENAR == False:\n",
        "     alexnet.load_state_dict(torch.load(ENLLAC_PESOS))\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    return alexnet,loss_fn\n",
        "  elif number == 1:\n",
        "    if ENTRENAR:\n",
        "      vgg = models.vgg16(weights = models.VGG16_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "      vgg = models.vgg16()\n",
        "\n",
        "    vgg.classifier[6] = nn.Linear(in_features=4096, out_features=1)\n",
        "\n",
        "    if ENTRENAR == False:\n",
        "      vgg.load_state_dict(torch.load(ENLLAC_PESOS))\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    return vgg,loss_fn\n",
        "  elif number == 2:\n",
        "    if ENTRENAR:\n",
        "      resnet = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "    else:\n",
        "      resnet = models.resnet18()\n",
        "\n",
        "    resnet.fc = nn.Linear(in_features=512, out_features=1)\n",
        "    if ENTRENAR == False:\n",
        "      resnet.load_state_dict(torch.load(ENLLAC_PESOS))\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    return resnet,loss_fn\n",
        "  elif number == 3:\n",
        "    unet = UNet(3,1)\n",
        "    if ENTRENAR == False:\n",
        "      unet.load_state_dict(torch.load(ENLLAC_PESOS))\n",
        "\n",
        "    loss_fn = DiceLoss()\n",
        "    return unet,loss_fn\n",
        "  elif number == 5:\n",
        "    if ENTRENAR:\n",
        "      model = YOLO('yolov5n.pt')\n",
        "    else:\n",
        "      model = YOLO(ENLLAC_PESOS)\n",
        "    return model,None\n",
        "  elif number == 4: #model propi\n",
        "    model = nn.Sequential(\n",
        "\n",
        "        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "        nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "        nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "        nn.AdaptiveAvgPool2d(output_size=(6, 6)),\n",
        "\n",
        "        nn.Flatten(),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(9216, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(4096, 1024),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(1024, 1024),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(1024, 1)\n",
        "    )\n",
        "\n",
        "    if ENTRENAR == False:\n",
        "      model.load_state_dict(torch.load(ENLLAC_PESOS))\n",
        "\n",
        "    return model, nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "ySRMpg9sf9O2"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "if ENTRENAR:\n",
        "  EPOCHS = 15\n",
        "else:\n",
        "  EPOCHS = 1\n",
        "\n",
        "\n",
        "architecrures_inv = {0: 'alexnet',1:'vgg',2:'resnet',3:'unet',4:'propi',5:'yolo',6:'rcnn'}\n",
        "projectes = ['caltech101-Alexnet','caltech101-Vgg','caltech101-ResNet','caltech101-unet','caltech101-Propi']\n",
        "\n",
        "\n",
        "training.__setmodel__(MODEL) #donam al conjunt quin model esteim empleant\n",
        "validation.__setmodel__(MODEL)\n",
        "testing.__setmodel__(MODEL)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(training, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation, batch_size=validation.__len__(), shuffle=True)\n",
        "testing_loader = torch.utils.data.DataLoader(testing, batch_size=testing.__len__(), shuffle=True)\n",
        "\n",
        "model,loss_fn = pick_algorithm(MODEL)\n",
        "model.to(device)\n",
        "\n",
        "learning_rate = 0.00015\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#inicialitzam weights and biases\n",
        "if WEIGHTSANDBIASES:\n",
        "  if MODEL != architectures['yolo']:\n",
        "    wandb.init(\n",
        "            project=projectes[MODEL],\n",
        "            config={\n",
        "                \"epochs\": EPOCHS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"lr\": learning_rate,\n",
        "                \"trsize\":training.__len__(),\n",
        "                \"trdist\":TRAINING,\n",
        "                \"vsize\":validation.__len__(),\n",
        "                \"vdist\":VAL,\n",
        "                \"duplicated\":DUPLICAR_DADES,\n",
        "                \"dropout\": 0.1\n",
        "                })\n",
        "\n",
        "\n",
        "    config = wandb.config\n",
        "\n",
        "test_target = None\n",
        "test_output = None\n",
        "\n",
        "#veure_imatges(training,std,mean)"
      ],
      "metadata": {
        "id": "rjilieRW5Rzj"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model,loss_fn,dataloader,optimizer,epoch):\n",
        "\n",
        "  batch_num = 1\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  train_f1 = 0\n",
        "  train_recall = 0\n",
        "  train_precision = 0\n",
        "\n",
        "  example_ct = 0\n",
        "\n",
        "  for batch_num, (input_img, target) in tqdm(enumerate(dataloader), desc=f\"Batches (Època {epoch})\"):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #extreu_classes(target.to(device))\n",
        "\n",
        "        output = model(input_img.to(device))\n",
        "\n",
        "        #print(target.shape)\n",
        "        # print(target)\n",
        "        #print(output.shape)\n",
        "        # print(output)\n",
        "\n",
        "\n",
        "\n",
        "        if MODEL != architectures['unet']:\n",
        "          target = target.float().unsqueeze(1)\n",
        "        else:\n",
        "          target = target.to(dtype=torch.float32)\n",
        "\n",
        "\n",
        "        target = target.to(device)\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          optimizer.step()\n",
        "\n",
        "\n",
        "        #print(f\"Pèrdua entrenament batch: {batch_num} epoch: {epoch+1}  train_loss: {loss.item()}\")\n",
        "        model.eval()\n",
        "\n",
        "        # print(\"Target unique values and counts:\", torch.unique(target, return_counts=True))\n",
        "        # print(\"Output unique values and counts:\", torch.unique((output > 0.5).int(), return_counts=True))\n",
        "\n",
        "\n",
        "        if MODEL == architectures['unet']:\n",
        "            output = (output.view(-1).cpu().detach().numpy() > 0.5).astype(int)\n",
        "            target = target.view(-1).cpu().detach().numpy()\n",
        "        else:\n",
        "          output = torch.sigmoid(output)\n",
        "          output = (output.cpu().detach().numpy() > 0.5).astype(int)\n",
        "          target = target.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "        y_class_predict = output\n",
        "\n",
        "\n",
        "\n",
        "        a= accuracy_score(target,y_class_predict)\n",
        "        b= f1_score(target,y_class_predict,zero_division=1)\n",
        "        c= recall_score(target,y_class_predict,zero_division=1)\n",
        "        d= precision_score(target,y_class_predict,zero_division=1)\n",
        "        e = loss.item()\n",
        "\n",
        "\n",
        "\n",
        "        train_acc += a\n",
        "        train_f1 += b\n",
        "        train_recall += c\n",
        "        train_precision += d\n",
        "        train_loss += e\n",
        "\n",
        "        example_ct += len(input_img)\n",
        "\n",
        "  return train_acc,train_f1,train_recall,train_precision,train_loss"
      ],
      "metadata": {
        "id": "itVzlC3ltGMi"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def validate(model,data_loader,loss_fn):\n",
        "\n",
        "  val_acc = 0\n",
        "  val_recall = 0\n",
        "  val_f1 = 0\n",
        "  val_loss = 0\n",
        "  val_precision = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch_num, (input_img, target) in enumerate(data_loader):\n",
        "\n",
        "\n",
        "            output = model(input_img.to(device))\n",
        "            target = target.to(device)\n",
        "\n",
        "            if MODEL != architectures['unet']:\n",
        "              target = target.float().unsqueeze(1)\n",
        "            else:\n",
        "              target = target.to(dtype=torch.float32)\n",
        "\n",
        "            loss = loss_fn(output, target)\n",
        "\n",
        "\n",
        "\n",
        "            if MODEL == architectures['unet']:\n",
        "              output = (output.view(-1).cpu().detach().numpy() > 0.5).astype(int)\n",
        "              target = target.view(-1).cpu().detach().numpy()\n",
        "            else:\n",
        "              output = torch.sigmoid(output)\n",
        "              output = (output.cpu().detach().numpy() > 0.5).astype(int)\n",
        "              target = target.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "            y_class_predict = output\n",
        "\n",
        "            val_acc  += accuracy_score(target,y_class_predict)\n",
        "            val_f1 += f1_score(target,y_class_predict,zero_division=1)\n",
        "            val_recall += recall_score(target,y_class_predict,zero_division=1)\n",
        "            val_precision += precision_score(target,y_class_predict,zero_division=1)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "  return val_acc,val_f1,val_recall,val_precision,val_loss"
      ],
      "metadata": {
        "id": "kwRzbZ92wUpk"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model,data_loader,loss_fn):\n",
        "\n",
        "  global test_target\n",
        "  global test_output\n",
        "\n",
        "\n",
        "  test_acc = 0\n",
        "  test_recall = 0\n",
        "  test_f1 = 0\n",
        "  test_loss = 0\n",
        "  test_precision = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch_num, (input_img, target) in enumerate(data_loader):\n",
        "\n",
        "\n",
        "            output = model(input_img.to(device))\n",
        "            target = target.to(device)\n",
        "\n",
        "            if MODEL != architectures['unet']:\n",
        "              target = target.float().unsqueeze(1)\n",
        "            else:\n",
        "              target = target.to(dtype=torch.float32)\n",
        "\n",
        "            loss = loss_fn(output, target)\n",
        "\n",
        "\n",
        "\n",
        "            if MODEL == architectures['unet']:\n",
        "\n",
        "              output = (output.view(-1).cpu().detach().numpy() > 0.5).astype(int)\n",
        "              target = target.view(-1).cpu().detach().numpy()\n",
        "\n",
        "              test_output = output\n",
        "              test_target = target\n",
        "\n",
        "            else:\n",
        "              output = torch.sigmoid(output)\n",
        "              output = (output.cpu().detach().numpy() > 0.5).astype(int)\n",
        "              target = target.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "            y_class_predict = output\n",
        "\n",
        "            test_acc  += accuracy_score(target,y_class_predict)\n",
        "            test_f1 += f1_score(target,y_class_predict,zero_division=1)\n",
        "            test_recall += recall_score(target,y_class_predict,zero_division=1)\n",
        "            test_precision += precision_score(target,y_class_predict,zero_division=1)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "  return test_acc,test_f1,test_recall,test_precision,test_loss"
      ],
      "metadata": {
        "id": "MFvVoKqiGesu"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prova():\n",
        "  t_loss = np.zeros(EPOCHS)\n",
        "  v_loss = np.zeros(EPOCHS)\n",
        "  acc_t = np.zeros(EPOCHS) #accuracy\n",
        "  acc_v = np.zeros(EPOCHS)\n",
        "  f1_t = np.zeros(EPOCHS) #f1\n",
        "  f1_v = np.zeros(EPOCHS)\n",
        "  recall_t = np.zeros(EPOCHS) #recall\n",
        "  recall_v = np.zeros(EPOCHS)\n",
        "  precision_t = np.zeros(EPOCHS)\n",
        "  precision_v = np.zeros(EPOCHS) #precisió\n",
        "\n",
        "  pbar = tqdm(range(1, EPOCHS + 1))  # tdqm permet tenir text dinàmic\n",
        "\n",
        "  for epoch in pbar:\n",
        "\n",
        "    if ENTRENAR:\n",
        "      train_acc,train_f1,train_recall,train_precision,train_loss = fit(model,loss_fn,train_loader,optimizer,epoch)\n",
        "\n",
        "    val_acc,val_f1,val_recall,val_precision,val_loss = validate(model,validation_loader,loss_fn)\n",
        "\n",
        "    test_acc,test_f1,test_recall,test_precision,test_loss = test(model,testing_loader,loss_fn)\n",
        "\n",
        "    if WEIGHTSANDBIASES:\n",
        "\n",
        "      training_metrics = {\"train/train_loss\": train_loss/len(train_loader),\n",
        "                      \"train/train_acc\":train_acc/len(train_loader),\n",
        "                      \"train/train_f1\":train_f1/len(train_loader),\n",
        "                      \"train/train_recall\":train_recall/len(train_loader),\n",
        "                      \"train/train_precision\":train_precision/len(train_loader)}\n",
        "\n",
        "      val_metrics = {\"val/val_loss\": val_loss/len(validation_loader),\n",
        "                    \"val/val_acc\":val_acc/len(validation_loader),\n",
        "                    \"val/val_f1\":val_f1/len(validation_loader),\n",
        "                    \"val/val_recall\": val_recall/len(validation_loader),\n",
        "                    \"val/val_precision\": val_precision/len(validation_loader)}\n",
        "\n",
        "      testing_metrics = {\"test/test_loss\": test_loss/len(testing_loader),\n",
        "                        \"test/test_acc\": test_acc/len(testing_loader),\n",
        "                        \"test/test_f1\":test_f1/len(testing_loader),\n",
        "                        \"test/test_recall\": test_recall/len(testing_loader),\n",
        "                          \"test/test_precision\": test_precision/len(testing_loader)}\n",
        "\n",
        "\n",
        "      #Guardam els resultats a weights and biases\n",
        "\n",
        "      wandb.log({**training_metrics, **val_metrics,**testing_metrics})\n",
        "\n",
        "      torch.save(model, \"my_model.pt\")\n",
        "      wandb.log_model(\"./my_model.pt\", architecrures_inv[MODEL], aliases=[f\"epoch-{epoch+1}\"])\n",
        "\n",
        "    if ENTRENAR:\n",
        "\n",
        "      torch.save(model.state_dict(), 'my_model.pt')\n",
        "\n",
        "      train_loss /= len(train_loader)\n",
        "      t_loss[epoch - 1] = train_loss\n",
        "\n",
        "      train_acc /= len(train_loader)\n",
        "      acc_t[epoch - 1] = train_acc\n",
        "\n",
        "      train_f1 /= len(train_loader)\n",
        "      f1_t[epoch - 1] = train_f1\n",
        "\n",
        "      train_recall /= len(train_loader)\n",
        "      recall_t[epoch - 1] = train_recall\n",
        "\n",
        "      train_precision /= len(train_loader)\n",
        "      precision_t[epoch-1] = train_precision\n",
        "\n",
        "      print(f\"Pèrdua entrenament epoch: {epoch}  train_loss: {train_loss}\")\n",
        "      print(f\"Accuracy train epoch: {epoch}  train_acc: {train_acc}\")\n",
        "      print(f\"F1 train epoch: {epoch}  train_f1: {train_f1}\")\n",
        "      print(f\"Recall train epoch: {epoch}  train_recall: {train_recall}\")\n",
        "      print(f\"Precision train epoch: {epoch}  train_recall: {train_precision}\")\n",
        "\n",
        "    val_loss /= len(validation_loader)\n",
        "    v_loss[epoch - 1] = val_loss\n",
        "\n",
        "    val_acc /= len(validation_loader)\n",
        "    acc_v[epoch - 1] = val_acc\n",
        "\n",
        "    val_f1 /= len(validation_loader)\n",
        "    f1_v[epoch - 1] = val_f1\n",
        "\n",
        "    val_recall /= len(validation_loader)\n",
        "    recall_v[epoch - 1] = val_recall\n",
        "\n",
        "    val_precision /= len(validation_loader)\n",
        "    precision_v[epoch-1] = val_precision\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print(f\"Pèrdua validació epoch: {epoch}  val_loss: {val_loss}\")\n",
        "    print(f\"Accuracy val epoch: {epoch}  val_acc: {val_acc}\")\n",
        "    print(f\"F1 val epoch: {epoch}  val_f1: {val_f1}\")\n",
        "    print(f\"Recall val epoch: {epoch}  val_reall: {val_recall}\")\n",
        "    print(f\"Precision val epoch: {epoch}  val_reall: {val_precision}\")\n",
        "\n",
        "    test_loss /= len(testing_loader)\n",
        "\n",
        "    test_acc /= len(testing_loader)\n",
        "\n",
        "    test_f1 /= len(testing_loader)\n",
        "\n",
        "    test_recall /= len(testing_loader)\n",
        "\n",
        "    test_precision /= len(testing_loader)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print(f\"Pèrdua testing epoch: {epoch}  test_loss: {test_loss}\")\n",
        "    print(f\"Accuracy test epoch: {epoch}  test_acc: {test_acc}\")\n",
        "    print(f\"F1 test epoch: {epoch}  test_f1: {test_f1}\")\n",
        "    print(f\"Recall test epoch: {epoch}  test_reall: {test_recall}\")\n",
        "    print(f\"Precision test epoch: {epoch}  test_reall: {test_precision}\")\n",
        "\n",
        "  if WEIGHTSANDBIASES:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "RlbcPphkxdhz"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "#Per a visualitzar les màsqueres que s'han predit al testing\n",
        "def visualitza_resultats():\n",
        "    test_output_images = test_output.reshape(testing.__len__(), 224, 224)\n",
        "    test_target_images = test_target.reshape(testing.__len__(), 224, 224)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.axis('off')\n",
        "\n",
        "    for i in range(testing.__len__()):\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        axs[0].cla()\n",
        "        axs[1].cla()\n",
        "\n",
        "        axs[0].imshow(test_output_images[i], cmap='gray', vmin=0, vmax=1)\n",
        "        axs[1].imshow(test_target_images[i], cmap='gray', vmin=0, vmax=1)\n",
        "\n",
        "        axs[0].set_title(f\"Output {i+1}\")\n",
        "        axs[1].set_title(f\"Target {i+1}\")\n",
        "\n",
        "        display(fig)\n",
        "        plt.pause(1)\n",
        "\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "65YoY2jTIkMi"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL == architectures['unet']:\n",
        "  prova()\n",
        "  #visualitza_resultats() #visualitza les màsqueres previstes\n",
        "\n",
        "elif MODEL != architectures['yolo']: #per als models de classificació\n",
        "  torch.use_deterministic_algorithms(False)\n",
        "  prova()\n",
        "else: #yolo\n",
        "\n",
        "  if ENTRENAR:\n",
        "    train_results = model.train(\n",
        "        data=\"/content/data.yaml\",\n",
        "        epochs=100,\n",
        "        imgsz=288,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "  metrics = model.val()\n",
        "  test_metrics = model.val(split='test')\n",
        "\n",
        "  results = model(X_test[0])\n",
        "  results[0].show()\n",
        "\n",
        "\n",
        "  path = model.export(format=\"onnx\")"
      ],
      "metadata": {
        "id": "ghEYwyFzJHA6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "4381d970ca8b4b839ad2f4bebba2ac5f",
            "8ea02427253e4c6a859e9c25ed5d30bd",
            "b965c7ff24fd426fa70b13dfd12bb3a3",
            "d1772f7e65cf4024985742cfbba1fe75",
            "e60823f17bdf4fbeaf2890e6d274b47b",
            "67e0ccce669e4308850432721160c192",
            "f974a9d67e854545862265b71fd10e0d",
            "4ada1a26c27d46f59fe2b4ff6131e82c",
            "77d5ea6c55ba462d9dfe2a8067ff02d8",
            "79f043c95dfa49aeae24cd900d7e4291",
            "3ad593ed7eba401fbf4dd857dd851482"
          ]
        },
        "outputId": "abe193cc-2665-46cc-f4b3-b4cc562d0b37"
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4381d970ca8b4b839ad2f4bebba2ac5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Pèrdua validació epoch: 1  val_loss: 0.0004985051928088069\n",
            "Accuracy val epoch: 1  val_acc: 1.0\n",
            "F1 val epoch: 1  val_f1: 1.0\n",
            "Recall val epoch: 1  val_reall: 1.0\n",
            "Precision val epoch: 1  val_reall: 1.0\n",
            "\n",
            "\n",
            "Pèrdua testing epoch: 1  test_loss: 0.0008537602843716741\n",
            "Accuracy test epoch: 1  test_acc: 1.0\n",
            "F1 test epoch: 1  test_f1: 1.0\n",
            "Recall test epoch: 1  test_reall: 1.0\n",
            "Precision test epoch: 1  test_reall: 1.0\n"
          ]
        }
      ]
    }
  ]
}