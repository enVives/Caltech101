{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMghP82Ing6f3AYB6Bdrwha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bcf3ee02e2ef49259d662e378a402447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59a3b9d2835f44f98e114312757cda3f",
              "IPY_MODEL_73b5716e34b541c1b3eead2e429644ef",
              "IPY_MODEL_9f241b029bf345c58566f0a372c14532"
            ],
            "layout": "IPY_MODEL_fb318a5b41404f598068ad3d7c81fd4d"
          }
        },
        "59a3b9d2835f44f98e114312757cda3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_291608ef6f1c4723bf851f6af653a83e",
            "placeholder": "​",
            "style": "IPY_MODEL_411444cce646483ab040db72b3b0add0",
            "value": "  0%"
          }
        },
        "73b5716e34b541c1b3eead2e429644ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1122b9d3e45046daa47aef070d175b09",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d21dd9dd8594459aa08255fe6178cc6",
            "value": 0
          }
        },
        "9f241b029bf345c58566f0a372c14532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_230768c864ef47479b707226b3a9ee2a",
            "placeholder": "​",
            "style": "IPY_MODEL_b919b93e81df439d9ba8235e56f06b21",
            "value": " 0/20 [00:00&lt;?, ?it/s]"
          }
        },
        "fb318a5b41404f598068ad3d7c81fd4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "291608ef6f1c4723bf851f6af653a83e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "411444cce646483ab040db72b3b0add0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1122b9d3e45046daa47aef070d175b09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d21dd9dd8594459aa08255fe6178cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "230768c864ef47479b707226b3a9ee2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b919b93e81df439d9ba8235e56f06b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "137dc0c7e32741e0983fe326ec427d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9124bf926a6e4452b2b861ac298f8f05",
              "IPY_MODEL_625e557fd71949b1ade31d9f56b92733",
              "IPY_MODEL_6bd3317251e049d1918f1350d15b4f67"
            ],
            "layout": "IPY_MODEL_fa8d86d8230949e2aadad30a958b6f57"
          }
        },
        "9124bf926a6e4452b2b861ac298f8f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4cf83b43557468e8b36efe92cf83dd8",
            "placeholder": "​",
            "style": "IPY_MODEL_4e5289819bff40f4b96042ae560b7369",
            "value": "Batches (Època 1): "
          }
        },
        "625e557fd71949b1ade31d9f56b92733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28d394d3e169421da83ddfbaf82cb601",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4059b9a343ca4bf6af26665712340e2e",
            "value": 0
          }
        },
        "6bd3317251e049d1918f1350d15b4f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ca6df12340d4feb83ca181b12e81f90",
            "placeholder": "​",
            "style": "IPY_MODEL_5792c6f4074f413da151de03d1d9b6d9",
            "value": " 0/? [00:00&lt;?, ?it/s]"
          }
        },
        "fa8d86d8230949e2aadad30a958b6f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4cf83b43557468e8b36efe92cf83dd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e5289819bff40f4b96042ae560b7369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28d394d3e169421da83ddfbaf82cb601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4059b9a343ca4bf6af26665712340e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ca6df12340d4feb83ca181b12e81f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5792c6f4074f413da151de03d1d9b6d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enVives/Caltech101/blob/main/Caltech101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "TRqBiAE19FY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651d3a83-629d-4b2c-b08c-16094c1ad528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ],
      "source": [
        "from itertools import filterfalse\n",
        "from collections import OrderedDict\n",
        "import torch,torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import cv2\n",
        "import time\n",
        "import os\n",
        "import wandb\n",
        "import pylab as pl\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms,models\n",
        "from torch import nn\n",
        "from IPython.display import clear_output,display\n",
        "from torch.utils.data import DataLoader, random_split,Subset,Dataset\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from glob import glob\n",
        "\n",
        "ROOT = './sample_data/'\n",
        "CLASS_A = 'cougar_body'\n",
        "CLASS_B = 'windsor_chair'\n",
        "\n",
        "DOWNLOAD = np.False_\n",
        "wandb.login()\n",
        "#471be466c8949671a46c67e7aad0d5a0ac8c9dad\n",
        "#!rm -rf /content/sample_data/*\n",
        "# torch.cuda.default_stream(torch.device('cuda'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_mean_std(dataset):\n",
        "  dataloader = DataLoader(dataset,batch_size = 1,shuffle=FALSE)\n",
        "  mean = torch.zeros(3)\n",
        "  std = torch.zeros(3)\n",
        "  samples = 0\n",
        "\n",
        "  for image,_ in dataloader:\n",
        "    mean += image.mean(dim=[0, 2, 3])  # Mean for each channel [batchsize,channels,height,width]\n",
        "    std += image.std(dim=[0, 2, 3])    # Std for each channel\n",
        "    samples += 1\n",
        "\n",
        "  mean /= samples\n",
        "  std /= samples\n",
        "\n",
        "  print(mean)\n",
        "  print(std)"
      ],
      "metadata": {
        "id": "4JYb0AOJbCdC"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import NONE\n",
        "\n",
        "class Formes(Dataset):\n",
        "    def __init__(self, paths, labels= None, transforms = None,annotations = None,transform_mask = None):\n",
        "        self.images = paths\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "        self.annotations = annotations\n",
        "        self.transforms_mask = transform_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __setmodel__(self,model):\n",
        "        self.model = model\n",
        "\n",
        "    def __getdist__(self):\n",
        "      return pd.Series(self.labels).value_counts()\n",
        "\n",
        "    def __showcontours__(self,index):\n",
        "      path = self.images[index]\n",
        "      image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "      image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      plt.imshow(image)\n",
        "\n",
        "      mat_data = scipy.io.loadmat(self.annotations[index])\n",
        "        #print(\"Keys in MAT file:\", polygons_data.keys())\n",
        "\n",
        "      polygons_data = mat_data['obj_contour']\n",
        "\n",
        "      x_points = polygons_data[0]\n",
        "      y_points = polygons_data[1]\n",
        "\n",
        "      contour_points = np.array(list(zip(x_points,y_points)))\n",
        "\n",
        "      plt.plot(contour_points[:, 0], contour_points[:, 1], '-r')  # Red contour line\n",
        "      plt.scatter(contour_points[:, 0], contour_points[:, 1], c='blue')  # Optional: Mark contour points\n",
        "      plt.show()\n",
        "\n",
        "    def __getpureimage__(self,index):\n",
        "      path = self.images[index]\n",
        "      image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "      image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "      return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      path = self.images[index]\n",
        "      label = self.labels[index]\n",
        "\n",
        "      if self.annotations != None:\n",
        "        mat_data = scipy.io.loadmat(self.annotations[index])\n",
        "        #print(\"Keys in MAT file:\", polygons_data.keys())\n",
        "\n",
        "        polygons_data = mat_data['obj_contour']\n",
        "        boundingbox_data = mat_data['box_coord']\n",
        "\n",
        "        x_points = polygons_data[0]\n",
        "        y_points = polygons_data[1]\n",
        "\n",
        "        contour_points = list(zip(x_points,y_points))\n",
        "        contour = np.array(contour_points, dtype=np.int32)\n",
        "\n",
        "        #print(contour_points)\n",
        "\n",
        "\n",
        "      image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "      image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      if self.model == 3:\n",
        "        mask = np.zeros(image.size, dtype=np.uint8)\n",
        "        cv2.fillPoly(mask, [contour], color=255)  # White filled polygon\n",
        "        mask = Image.fromarray(mask)  # Convert NumPy array to PIL Image\n",
        "        mask_resized = self.transforms_mask(mask)\n",
        "        mask_resized = (mask_resized > 0.5).int()\n",
        "\n",
        "        image = self.transforms(image)\n",
        "\n",
        "        return image, mask_resized\n",
        "\n",
        "      image = self.transforms(image)\n",
        "\n",
        "      return image, label"
      ],
      "metadata": {
        "id": "8imK0xLhEnm1"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(sett):\n",
        "  labels = np.array([])\n",
        "  frequency = np.zeros(101)\n",
        "  for _,label in sett:\n",
        "    labels = np.append(labels,label)\n",
        "    frequency[label] += 1\n",
        "  return labels.astype(int),frequency"
      ],
      "metadata": {
        "id": "ifL4lRvNSHlm"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from pickle import FALSE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TRAINING = 0.80\n",
        "VAL = 0.10\n",
        "TESTING = 0.10\n",
        "\n",
        "#[0.5459, 0.5288, 0.5022]\n",
        "#[0.2424, 0.2393, 0.2409]\n",
        "mean = torch.tensor([0.485,0.456,0.406])\n",
        "std = torch.tensor([0.229,0.224,0.225])\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "transform_masks = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "    #transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),  # Ensure all images are RGB\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.Caltech101(root= ROOT,download=DOWNLOAD,transform=transform)\n",
        "img_class_1 = sorted(glob('/content/sample_data/caltech101/101_ObjectCategories/cougar_body/*'))\n",
        "img_class_2 = sorted(glob('/content/sample_data/caltech101/101_ObjectCategories/windsor_chair/*'))\n",
        "\n",
        "img_annotations_class_1 = sorted(glob('/content/sample_data/caltech101/Annotations/cougar_body/*'))\n",
        "img_annotations_class_2 = sorted(glob('/content/sample_data/caltech101/Annotations/windsor_chair/*'))\n",
        "\n",
        "\n",
        "img_files = img_class_1 +  img_class_2\n",
        "img_annotations = img_annotations_class_1 + img_annotations_class_2\n",
        "\n",
        "labels = []\n",
        "for img_path in img_files:\n",
        "  label = img_path.split(os.path.sep)[-2]\n",
        "  labels.append(label)\n",
        "\n",
        "lb = LabelEncoder()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "weights = 1.0 / pd.Series(labels).value_counts()\n",
        "weights = weights / weights.sum()\n",
        "weights = torch.tensor(weights,dtype=torch.float32)\n",
        "\n",
        "X_train, X_test, y_train, y_test, annotations_train, annotations_test = train_test_split(\n",
        "    img_files, labels, img_annotations, test_size=TESTING, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Split the training set further into training and validation sets\n",
        "X_train, X_val, y_train, y_val, annotations_train, annotations_val = train_test_split(\n",
        "    X_train, y_train, annotations_train, test_size=VAL/(TRAINING+VAL), random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "training = Formes(X_train,y_train,transform,annotations_train,transform_masks)\n",
        "testing = Formes(X_test,y_test,transform,annotations_test,transform_masks)\n",
        "validation = Formes(X_val,y_val,transform,annotations_val,transform_masks)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#training,validation,testing = random_split(dataset,[train_size,validation_size,len(dataset)-train_size-validation_size])"
      ],
      "metadata": {
        "id": "5e-PEAT6NDpO"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"LONGITUD SET DE TRAINING: {training.__len__()}\")\n",
        "with open(\"training.txt\", \"w\") as file:\n",
        "    for item in training.__getdist__()/training.__len__():\n",
        "        file.write(f\"{item}\\n\")\n",
        "print(f\"LONGITUD SET DE VALIDATION: {validation.__len__()}\")\n",
        "with open(\"valid.txt\", \"w\") as file:\n",
        "    for item in validation.__getdist__()/validation.__len__():\n",
        "        file.write(f\"{item}\\n\")\n",
        "print(f\"LONGITUD SET DE TESTING: {testing.__len__()}\")"
      ],
      "metadata": {
        "id": "npbH2OEvRSVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8999ec-b129-409f-be5d-b295c1b2c288"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LONGITUD SET DE TRAINING: 81\n",
            "LONGITUD SET DE VALIDATION: 11\n",
            "LONGITUD SET DE TESTING: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ara ja sabem que hem de predir si una imatge pertany a les classes 25 o 99"
      ],
      "metadata": {
        "id": "riWfhpfjmgeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def veure_imatges(train_data,std,mean):\n",
        "  for i in range(len(train_data)):\n",
        "    imatge,label = train_data[i]\n",
        "\n",
        "    print(imatge.ndimension())\n",
        "    print(imatge.shape)\n",
        "\n",
        "    imatge = imatge * (std[:, None, None]*255) + (mean[:, None, None]*255)\n",
        "    # Convert the tensor back to a NumPy array\n",
        "    img_numpy = imatge.permute(1, 2, 0).numpy()  # Change from (C, H, W) to (H, W, C)\n",
        "    cv2_imshow(img_numpy)\n",
        "    time.sleep(5)\n",
        "    clear_output(wait=True)"
      ],
      "metadata": {
        "id": "q6rpD5v8EW8G"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "\n",
        "        ## CODER\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        ## DECODER\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2,\n",
        "                                          stride=2)  # Empra aquesta capa com exemple\n",
        "        self.decoder4 = UNet._block(features * 16, features * 8, name=\"dec4\")\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = UNet._block(features * 8, features * 4, name=\"dec3\")\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = UNet._block(features * 4, features * 2, name=\"dec2\")\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.final = nn.Conv2d(\n",
        "            in_channels=features,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=1,\n",
        "            padding=0,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec1 = self.upconv4(bottleneck)\n",
        "        dec1 = torch.cat((dec1, enc4), dim=1)\n",
        "        dec2 = self.decoder4(dec1)\n",
        "\n",
        "        dec2 = self.upconv3(dec2)\n",
        "        dec2 = torch.cat((dec2, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec2)\n",
        "\n",
        "        dec3 = self.upconv2(dec3)\n",
        "        dec3 = torch.cat((dec3, enc2), dim=1)\n",
        "        dec4 = self.decoder2(dec3)\n",
        "\n",
        "        dec4 = self.upconv1(dec4)\n",
        "        dec4 = torch.cat((dec4, enc1), dim=1)\n",
        "        dec5 = self.decoder1(dec4)\n",
        "        return torch.sigmoid(self.final(dec5))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (name + \"conv1\",\n",
        "                     nn.Conv2d(\n",
        "                         in_channels=in_channels,\n",
        "                         out_channels=features,\n",
        "                         kernel_size=3,\n",
        "                         padding=1,\n",
        "                         bias=False,\n",
        "                     ),\n",
        "                     ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (name + \"conv2\",\n",
        "                     nn.Conv2d(\n",
        "                         in_channels=features,\n",
        "                         out_channels=features,\n",
        "                         kernel_size=3,\n",
        "                         padding=1,\n",
        "                         bias=False,\n",
        "                     ),\n",
        "                     ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "id": "pUYwQPDSev7e"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = 0.0\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        print(y_pred)\n",
        "        print(y_true)\n",
        "        assert y_pred.size() == y_true.size()\n",
        "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
        "        y_true = y_true[:, 0].contiguous().view(-1)\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dsc = (2. * intersection + self.smooth) / (\n",
        "                y_pred.sum() + y_true.sum() + self.smooth\n",
        "        )\n",
        "        return 1. - dsc"
      ],
      "metadata": {
        "id": "Mi_tP2rWd50b"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pick_algorithm(number):\n",
        "  if number == 0:\n",
        "    alexnet = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1) #fine tuning\n",
        "    # Modify the classifier\n",
        "    alexnet.classifier[6] = nn.Linear(in_features=4096, out_features=1)\n",
        "\n",
        "    # alexnetmulticlass.classifier = nn.Sequential(\n",
        "    # torch.nn.Dropout(p=0.1),\n",
        "    # torch.nn.Linear(9216, 2048),\n",
        "    # nn.ReLU(inplace=True),\n",
        "    # torch.nn.Linear(2048, 1024),\n",
        "    # nn.ReLU(inplace=True),\n",
        "    # torch.nn.Linear(1024, 512),\n",
        "    # nn.ReLU(inplace=True),\n",
        "    # torch.nn.Linear(512, 1),  # Ja que tenim 10 classes.\n",
        "    # )\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    #loss_fn = nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "    return alexnet,loss_fn\n",
        "  elif number == 1:\n",
        "    vgg = models.vgg16(weights = models.VGG16_Weights.IMAGENET1K_V1)\n",
        "    vgg.classifier[6] = nn.Linear(in_features=4096, out_features=1)\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    return vgg,loss_fn\n",
        "  elif number == 2:\n",
        "    resnet = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "    resnet.classifier[6] = nn.Linear(in_features=4096, out_features=1)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    return resnet,loss_fn\n",
        "  elif number == 3:\n",
        "    unet = UNet(3,1)\n",
        "    loss_fn = DiceLoss()\n",
        "    return unet,loss_fn"
      ],
      "metadata": {
        "id": "ySRMpg9sf9O2"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 20\n",
        "PATIENCE = 1\n",
        "\n",
        "architectures = {'alexnet': 0,'vgg': 1,'resnet': 2,'unet':3}\n",
        "\n",
        "MODEL = architectures['unet']\n",
        "\n",
        "training.__setmodel__(MODEL)\n",
        "validation.__setmodel__(MODEL)\n",
        "testing.__setmodel__(MODEL)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(training, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation, batch_size=validation.__len__(), shuffle=True)\n",
        "testing_loader = torch.utils.data.DataLoader(testing, batch_size=testing.__len__(), shuffle=True)\n",
        "\n",
        "model,loss_fn = pick_algorithm(MODEL)\n",
        "model.to(device)\n",
        "\n",
        "learning_rate = 1e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "wandb.init(\n",
        "        project=\"caltech101-proves\",\n",
        "        config={\n",
        "            \"epochs\": 20,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"lr\": learning_rate,\n",
        "            \"tsize\":TRAINING,\n",
        "            \"vsize\":VAL,\n",
        "            \"weights\": True\n",
        "            })\n",
        "\n",
        "\n",
        "config = wandb.config\n",
        "#veure_imatges(training,std,mean)"
      ],
      "metadata": {
        "id": "rjilieRW5Rzj"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, target= next(iter(train_loader))\n",
        "print(img.shape)"
      ],
      "metadata": {
        "id": "wqzBODaIK61P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178abd14-eb61-41a9-b0db-5d8adbf930f3"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model,loss_fn,dataloader,optimizer,epoch,config):\n",
        "\n",
        "  batch_num = 1\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  train_f1 = 0\n",
        "  train_recall = 0\n",
        "  train_precision = 0\n",
        "\n",
        "  n_steps_per_epoch = math.ceil(len(dataloader) / config.batch_size)\n",
        "  example_ct = 0\n",
        "\n",
        "  for batch_num, (input_img, target) in tqdm(enumerate(dataloader), desc=f\"Batches (Època {epoch})\"):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #extreu_classes(target.to(device))\n",
        "\n",
        "        output = model(input_img.to(device))\n",
        "\n",
        "        #print(target.shape)\n",
        "        # print(target)\n",
        "        #print(output.shape)\n",
        "        # print(output)\n",
        "\n",
        "        if MODEL != architectures['unet']:\n",
        "          target = target.float().unsqueeze(1)\n",
        "\n",
        "\n",
        "        target = target.to(device)\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            optimizer.step()\n",
        "\n",
        "        #print(f\"Pèrdua entrenament batch: {batch_num} epoch: {epoch+1}  train_loss: {loss.item()}\")\n",
        "        model.eval()\n",
        "\n",
        "        output = torch.sigmoid(output)\n",
        "\n",
        "        output = (output.cpu().detach().numpy() > 0.5).astype(int)\n",
        "\n",
        "        y_class_predict = output\n",
        "        target = target.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "        a= accuracy_score(target,y_class_predict)\n",
        "        b= f1_score(target,y_class_predict,zero_division=1)\n",
        "        c= recall_score(target,y_class_predict,zero_division=1)\n",
        "        d= precision_score(target,y_class_predict,zero_division=1)\n",
        "        e = loss.item()\n",
        "\n",
        "        train_acc += a\n",
        "        train_f1 += b\n",
        "        train_recall += c\n",
        "        train_precision += d\n",
        "        train_loss += e\n",
        "\n",
        "        example_ct += len(input_img)\n",
        "\n",
        "  return train_acc,train_f1,train_recall,train_precision,train_loss"
      ],
      "metadata": {
        "id": "itVzlC3ltGMi"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model,data_loader,loss_fn,config):\n",
        "\n",
        "  val_acc = 0\n",
        "  val_recall = 0\n",
        "  val_f1 = 0\n",
        "  val_loss = 0\n",
        "  val_precision = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch_num, (input_img, target) in enumerate(data_loader):\n",
        "\n",
        "\n",
        "            output = model(input_img.to(device))\n",
        "            target = target.to(device)\n",
        "\n",
        "            if MODEL != architectures['unet']:\n",
        "              target = target.float().unsqueeze(1)\n",
        "\n",
        "            loss = loss_fn(output, target)\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "\n",
        "            output = (output.cpu().detach().numpy() > 0.5).astype(int)\n",
        "\n",
        "            y_class_predict = output\n",
        "            target = target.cpu().detach().numpy()\n",
        "\n",
        "            val_acc  += accuracy_score(target,y_class_predict)\n",
        "            val_f1 += f1_score(target,y_class_predict,zero_division=1)\n",
        "            val_recall += recall_score(target,y_class_predict,zero_division=1)\n",
        "            val_precision += precision_score(target,y_class_predict,zero_division=1)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "  return val_acc,val_f1,val_recall,val_precision,val_loss"
      ],
      "metadata": {
        "id": "kwRzbZ92wUpk"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def executa():\n",
        "t_loss = np.zeros(EPOCHS)\n",
        "v_loss = np.zeros(EPOCHS)\n",
        "acc_t = np.zeros(EPOCHS) #accuracy\n",
        "acc_v = np.zeros(EPOCHS)\n",
        "f1_t = np.zeros(EPOCHS) #f1\n",
        "f1_v = np.zeros(EPOCHS)\n",
        "recall_t = np.zeros(EPOCHS) #recall\n",
        "recall_v = np.zeros(EPOCHS)\n",
        "precision_t = np.zeros(EPOCHS)\n",
        "precision_v = np.zeros(EPOCHS) #precisió\n",
        "\n",
        "epochs_without_improvement = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "pbar = tqdm(range(1, EPOCHS + 1))  # tdqm permet tenir text dinàmic\n",
        "\n",
        "for epoch in pbar:\n",
        "\n",
        "\n",
        "  train_acc,train_f1,train_recall,train_precision,train_loss = fit(model,loss_fn,train_loader,optimizer,epoch,config)\n",
        "\n",
        "  val_acc,val_f1,val_recall,val_precision,val_loss = validate(model,validation_loader,loss_fn,config)\n",
        "\n",
        "  #test_acc,test_f1,test_recall,test_precision_test_loss = validate(model,testing_loader,loss_fn,config)\n",
        "\n",
        "  training_metrics = {\"train/train_loss\": train_loss/len(train_loader),\n",
        "                  \"train/train_acc\":train_acc/len(train_loader),\n",
        "                  \"train/train_f1\":train_f1/len(train_loader),\n",
        "                  \"train/train_recall\":train_recall/len(train_loader),\n",
        "                  \"train/train_precision\":train_precision/len(train_loader)}\n",
        "\n",
        "  val_metrics = {\"val/val_loss\": val_loss/len(validation_loader),\n",
        "                \"val/val_acc\":val_acc/len(validation_loader),\n",
        "                \"val/val_f1\":val_f1/len(validation_loader),\n",
        "                \"val/val_recall\": val_recall/len(validation_loader),\n",
        "                \"val/val_precision\": val_precision/len(validation_loader)}\n",
        "\n",
        "  # testing_metrics = {\"test/test_loss\": test_loss/len(testing_loader),\n",
        "  #                    \"test/test_acc\": test_acc/len(testing_loader),\n",
        "  #                    \"test/test_f1\":test_f1/len(testing_loader),\n",
        "  #                    \"test/test_recall\": test_recall/len(testing_loader),\n",
        "  #                     \"test/test_precision\": test_precision/len(testing_loader)}\n",
        "\n",
        "  #Early Stopping:\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      #epochs_without_improvement = 0\n",
        "  else:\n",
        "    print(\"Early stopping triggered!\")\n",
        "    break\n",
        "      #epochs_without_improvement += 1\n",
        "\n",
        "  #Saving the results\n",
        "\n",
        "  wandb.log({**training_metrics, **val_metrics})\n",
        "\n",
        "  torch.save(model, \"my_model.pt\")\n",
        "  wandb.log_model(\"./my_model.pt\", \"alexnet\", aliases=[f\"epoch-{epoch+1}\"])\n",
        "\n",
        "  train_loss /= len(train_loader)\n",
        "  t_loss[epoch - 1] = train_loss\n",
        "\n",
        "  train_acc /= len(train_loader)\n",
        "  acc_t[epoch - 1] = train_acc\n",
        "\n",
        "  train_f1 /= len(train_loader)\n",
        "  f1_t[epoch - 1] = train_f1\n",
        "\n",
        "  train_recall /= len(train_loader)\n",
        "  recall_t[epoch - 1] = train_recall\n",
        "\n",
        "  train_precision /= len(train_loader)\n",
        "  precision_t[epoch-1] = train_precision\n",
        "\n",
        "  print(f\"Pèrdua entrenament epoch: {epoch}  train_loss: {train_loss}\")\n",
        "  print(f\"Accuracy train epoch: {epoch}  train_acc: {train_acc}\")\n",
        "  print(f\"F1 train epoch: {epoch}  train_f1: {train_f1}\")\n",
        "  print(f\"Recall train epoch: {epoch}  train_recall: {train_recall}\")\n",
        "  print(f\"Precision train epoch: {epoch}  train_recall: {train_precision}\")\n",
        "\n",
        "  val_loss /= len(validation_loader)\n",
        "  v_loss[epoch - 1] = val_loss\n",
        "\n",
        "  val_acc /= len(validation_loader)\n",
        "  acc_v[epoch - 1] = val_acc\n",
        "\n",
        "  val_f1 /= len(validation_loader)\n",
        "  f1_v[epoch - 1] = val_f1\n",
        "\n",
        "  val_recall /= len(validation_loader)\n",
        "  recall_v[epoch - 1] = val_recall\n",
        "\n",
        "  val_precision /= len(validation_loader)\n",
        "  precision_v[epoch-1] = val_precision\n",
        "\n",
        "  print()\n",
        "  print()\n",
        "  print(f\"Pèrdua validació epoch: {epoch}  val_loss: {val_loss}\")\n",
        "  print(f\"Accuracy val epoch: {epoch}  val_acc: {val_acc}\")\n",
        "  print(f\"F1 val epoch: {epoch}  val_f1: {val_f1}\")\n",
        "  print(f\"Recall val epoch: {epoch}  val_reall: {val_recall}\")\n",
        "  print(f\"Precision val epoch: {epoch}  val_reall: {val_precision}\")\n",
        "\n",
        "  # if epochs_without_improvement >= PATIENCE:\n",
        "  #     print(\"Early stopping triggered!\")\n",
        "  #     break\n",
        "\n",
        "  # if val_acc > 0.9:\n",
        "  #   print(\"Early stopping triggered!\")\n",
        "  #   break\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "RlbcPphkxdhz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bcf3ee02e2ef49259d662e378a402447",
            "59a3b9d2835f44f98e114312757cda3f",
            "73b5716e34b541c1b3eead2e429644ef",
            "9f241b029bf345c58566f0a372c14532",
            "fb318a5b41404f598068ad3d7c81fd4d",
            "291608ef6f1c4723bf851f6af653a83e",
            "411444cce646483ab040db72b3b0add0",
            "1122b9d3e45046daa47aef070d175b09",
            "9d21dd9dd8594459aa08255fe6178cc6",
            "230768c864ef47479b707226b3a9ee2a",
            "b919b93e81df439d9ba8235e56f06b21",
            "137dc0c7e32741e0983fe326ec427d53",
            "9124bf926a6e4452b2b861ac298f8f05",
            "625e557fd71949b1ade31d9f56b92733",
            "6bd3317251e049d1918f1350d15b4f67",
            "fa8d86d8230949e2aadad30a958b6f57",
            "f4cf83b43557468e8b36efe92cf83dd8",
            "4e5289819bff40f4b96042ae560b7369",
            "28d394d3e169421da83ddfbaf82cb601",
            "4059b9a343ca4bf6af26665712340e2e",
            "3ca6df12340d4feb83ca181b12e81f90",
            "5792c6f4074f413da151de03d1d9b6d9"
          ]
        },
        "outputId": "5db302d2-505f-4071-e907-8d8009b3f275"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcf3ee02e2ef49259d662e378a402447"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches (Època 1): 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "137dc0c7e32741e0983fe326ec427d53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.6383, 0.5975, 0.5913,  ..., 0.6292, 0.6189, 0.5811],\n",
            "          [0.6320, 0.5267, 0.5127,  ..., 0.6159, 0.5399, 0.6154],\n",
            "          [0.6414, 0.5437, 0.6056,  ..., 0.5824, 0.6419, 0.6115],\n",
            "          ...,\n",
            "          [0.5887, 0.6370, 0.5586,  ..., 0.6908, 0.6580, 0.5757],\n",
            "          [0.6217, 0.6088, 0.6520,  ..., 0.6884, 0.6679, 0.5993],\n",
            "          [0.6092, 0.6167, 0.5185,  ..., 0.6657, 0.6751, 0.5827]]],\n",
            "\n",
            "\n",
            "        [[[0.5911, 0.5571, 0.5331,  ..., 0.5495, 0.6085, 0.5276],\n",
            "          [0.6130, 0.5345, 0.5541,  ..., 0.5702, 0.6193, 0.5728],\n",
            "          [0.6765, 0.5993, 0.6330,  ..., 0.6660, 0.6737, 0.4962],\n",
            "          ...,\n",
            "          [0.6274, 0.5607, 0.5904,  ..., 0.6398, 0.5497, 0.5459],\n",
            "          [0.6145, 0.4566, 0.5142,  ..., 0.5716, 0.5388, 0.5054],\n",
            "          [0.6685, 0.5406, 0.5763,  ..., 0.5372, 0.5858, 0.5908]]],\n",
            "\n",
            "\n",
            "        [[[0.5960, 0.5754, 0.5333,  ..., 0.5494, 0.6153, 0.5270],\n",
            "          [0.6242, 0.5426, 0.5372,  ..., 0.5455, 0.6257, 0.5886],\n",
            "          [0.6787, 0.6003, 0.6333,  ..., 0.7004, 0.6893, 0.5008],\n",
            "          ...,\n",
            "          [0.6167, 0.5812, 0.5396,  ..., 0.8046, 0.6817, 0.5507],\n",
            "          [0.6466, 0.4916, 0.5133,  ..., 0.6128, 0.6743, 0.5714],\n",
            "          [0.6771, 0.5092, 0.6098,  ..., 0.6669, 0.5912, 0.6111]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.5956, 0.5761, 0.5329,  ..., 0.5491, 0.6133, 0.5296],\n",
            "          [0.6248, 0.5413, 0.5368,  ..., 0.5584, 0.6176, 0.5850],\n",
            "          [0.6786, 0.5990, 0.6338,  ..., 0.6834, 0.6911, 0.5024],\n",
            "          ...,\n",
            "          [0.6181, 0.5801, 0.5406,  ..., 0.6838, 0.5668, 0.5212],\n",
            "          [0.6451, 0.4910, 0.5147,  ..., 0.6020, 0.5653, 0.5001],\n",
            "          [0.6766, 0.5081, 0.6094,  ..., 0.5500, 0.5732, 0.5920]]],\n",
            "\n",
            "\n",
            "        [[[0.6235, 0.5382, 0.5563,  ..., 0.5429, 0.6098, 0.5351],\n",
            "          [0.5783, 0.5255, 0.5710,  ..., 0.5692, 0.6133, 0.5555],\n",
            "          [0.6470, 0.5707, 0.5974,  ..., 0.6429, 0.6370, 0.5049],\n",
            "          ...,\n",
            "          [0.5829, 0.5748, 0.5349,  ..., 0.5764, 0.5261, 0.5680],\n",
            "          [0.5677, 0.5059, 0.5563,  ..., 0.5402, 0.5135, 0.5801],\n",
            "          [0.6175, 0.5914, 0.5990,  ..., 0.5809, 0.6036, 0.5865]]],\n",
            "\n",
            "\n",
            "        [[[0.7007, 0.6131, 0.6794,  ..., 0.6240, 0.6552, 0.6302],\n",
            "          [0.7463, 0.7008, 0.5921,  ..., 0.6415, 0.6653, 0.5518],\n",
            "          [0.7817, 0.6350, 0.6399,  ..., 0.6519, 0.7579, 0.5620],\n",
            "          ...,\n",
            "          [0.6364, 0.5808, 0.5602,  ..., 0.5947, 0.5658, 0.5659],\n",
            "          [0.6142, 0.6163, 0.6354,  ..., 0.6168, 0.5459, 0.5666],\n",
            "          [0.6466, 0.6325, 0.5529,  ..., 0.6237, 0.6215, 0.5986]]]],\n",
            "       device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
            "tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          ...,\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          ...,\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          ...,\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          ...,\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          ...,\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "        [[[0, 0, 0,  ..., 1, 1, 1],\n",
            "          [0, 0, 0,  ..., 1, 1, 1],\n",
            "          [0, 0, 0,  ..., 1, 1, 1],\n",
            "          ...,\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0],\n",
            "          [0, 0, 0,  ..., 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "unknown is not supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-258-dcae39f04218>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_recall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_recall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-256-f9a72ca8bfff>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, loss_fn, dataloader, optimizer, epoch, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_class_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_class_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_class_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattach_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: unknown is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# index = 10\n",
        "# _,mask = training.__getitem__(index)\n",
        "# print(label)\n",
        "\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Adjust figsize to control the size\n",
        "# # Display the mask\n",
        "# axes[0].imshow(mask.squeeze(),cmap=\"gray\")  # Assuming `mask` is grayscale\n",
        "# axes[0].set_title(\"Mask\")\n",
        "# axes[0].axis('off')  # Hide axes\n",
        "# # Display the pure image\n",
        "# axes[1].imshow(training.__getpureimage__(index))  # Assuming `pure_image` is RGB\n",
        "# axes[1].set_title(\"Pure Image\")\n",
        "# axes[1].axis('off')  # Hide axes\n",
        "# # Show the figure\n",
        "# plt.tight_layout()  # Optional: Adjust spacing between plots\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "NaIyx6ZvLPMf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}